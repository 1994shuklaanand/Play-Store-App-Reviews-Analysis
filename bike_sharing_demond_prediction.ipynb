{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "PH-0ReGfmX4f",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "JcMwzZxoAimU",
        "8G2x9gOozGDZ",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1994shuklaanand/Play-Store-App-Reviews-Analysis/blob/main/bike_sharing_demond_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - \n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - Team\n",
        "##### **Team Member 1 -** Anand Kumar\n",
        "##### **Team Member 2 -** Vinay Chaudhari\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bike sharing systems means that renting bicycles in which there is a process of obtaining membership, rental, and bike return is automatically. Using these systems, people are able rent a bike from a one location and return it to a different place on an as-needed basis.\n",
        "\n",
        "Currently, there are over 500 bike-sharing programs around the world. Several bike/scooter rides sharing facilities (e.g., Bird, Capital Bikeshare, Citi Bike) have started up lately especially in metropolitan cities like San Francisco, New York, Chicago and Los Angeles, and one of the most important problem from a business point of view is to predict the bike demand on any particular day.\n",
        "\n",
        "While having excess bikes results in wastage of resource (both with respect to bike maintenance and the land/bike stand required for parking and security), having fewer bikes leads to revenue loss (ranging from a short term loss due to missing out on immediate customers to potential longer term loss due to loss in future customer base), Thus, having an estimate on the demands would enable efficient functioning of these companies.\n",
        "\n",
        "The goal of this project is to combine the historical bike usage patterns with the weather data to forecast bike rental demand. The data set consists of hourly rental data spanning two years. The training set is comprised of the first 19 days of each month, while the test set is the 20th to the end of the month\n",
        "To build a machine learning model on this data, we first gathered and clean the data, and handled the null values, then we performed indepth EDA with visuals and we gathered many insights from our EDA. Then further on, we did data preprocessing.\n",
        "\n",
        "Then we split it into training and testing sets. Next, we choose a machine learning algorithm and use the training data to train the model. Finally, you we evaluated the model's performance on the testing data to see how well it is able to predict sales.\n",
        "\n",
        "There are many different machine learning algorithms that we used for this task, including Linear Regression, Decision trees, Random Forests, Light GBM and XGBOOST. It is also possible to use more advanced techniques, such as deep learning, to build a model on Bike Shairing Demond Prediction data.\n",
        "\n",
        "Overall, while building a machine learning model on Bike Shairing Demond Prediction data we applied combination of data processing, machine learning techniques, and model evaluation skills. It was a challenging task, but with the right approach, we were able to create a model that can accurately predict sales for a retail store chain."
      ],
      "metadata": {
        "id": "R2nE3Z2RNyFY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Currently Rental bikes are introduced in many urban cities for the enhancement of mobility comfort. It is important to make the rental bike available and accessible to the public at the right time as it lessens the waiting time. Eventually, providing the city with a stable supply of rental bikes becomes a major concern. The crucial part is the prediction of bike count required at each hour for the stable supply of rental bikes."
      ],
      "metadata": {
        "id": "GW5fnpr_ONhG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required. \n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits. \n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 20 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule. \n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# ML eveluation library\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "# ML Model implementation library\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.tree import plot_tree\n",
        "\n",
        "#figure plot library\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import warnings"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = ('/content/drive/MyDrive/Bike Sharing Predection/SeoulBikeData.csv')\n",
        "dataset = pd.read_csv(path, encoding = \"ISO-8859-1\")"
      ],
      "metadata": {
        "id": "BHTlwcxFPEgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# View top 5 dataset\n",
        "dataset.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View top 5 dataset\n",
        "dataset.tail()"
      ],
      "metadata": {
        "id": "gg19VqHsPmSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "print('Number of (rows, columns) are',dataset.shape)"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Date : year-month-day\n",
        "\n",
        "Rented Bike Count : Count of bikes rented at each hour\n",
        "\n",
        "Hour : Hour of the day\n",
        "\n",
        "Temperature(°C) : Temperature in Celsius\n",
        "\n",
        "Humidity(%) : Relative Humidity%\n",
        "\n",
        "Wind speed (m/s) : Average Speed of the wind(m/s)\n",
        "\n",
        "Visibility (10m) : 10meter\n",
        "\n",
        "Dew point temperature(°C) : Celsius\n",
        "\n",
        "Solar Radiation (MJ/m2) : Megajoules/meter*meter\n",
        "\n",
        "Rainfall(mm) : millimetre\n",
        "\n",
        "Snowfall (cm) : centimeter\n",
        "\n",
        "Seasons : Winter, Spring, Summer, Autumn\n",
        "\n",
        "Holiday : Holiday/No holiday\n",
        "\n",
        "Functioning Day : NoFunc(Non Functional Hours), Fun(Functional hours)"
      ],
      "metadata": {
        "id": "W-50zAuCQ4dm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset.info\n",
        "dataset.info()"
      ],
      "metadata": {
        "id": "qtBrnIB8RfrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value\n",
        "dataset.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is no Value Present into it."
      ],
      "metadata": {
        "id": "O6iSKmSNR8o5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "dataset.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can say that there is no value present in it."
      ],
      "metadata": {
        "id": "7DqscbRsSYSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "plt.figure(figsize=(14, 5))\n",
        "sns.heatmap(dataset.isnull(), cbar=True, yticklabels=False)\n",
        "plt.xlabel(\"column_name\", size=14, weight=\"bold\")\n",
        "plt.title(\"missing values in column\",fontweight=\"bold\",size=17)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Till now we get to know the following points about our datasets\n",
        "\n",
        "1. 'SeoulBikeData' is having 8760 rows and 14 columns and does not have any null value.\n",
        "2. There is no duplicate values present in both datasets.\n",
        "\n",
        "\n",
        "3. There is no null value present in both datasets.\n",
        "4.There are total 4 categorical features in 'SeoulBikeData' dataset namely : Date, Season, Holiday and Functioning Day.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KDN-WN4nS9pr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "dataset.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "dataset.describe(include='all').T"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description "
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have handled all the null values in our dataset, and created new variables using date column."
      ],
      "metadata": {
        "id": "yCAwErxxefgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.head()"
      ],
      "metadata": {
        "id": "_XHzDgccet_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "dataset.nunique()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable\n",
        "for i in dataset.columns.tolist():\n",
        "  print(f\"The Unique Values of', {i}, 'are:\", dataset[i].unique())\n",
        "  print()\n",
        "  print('--'*50)"
      ],
      "metadata": {
        "id": "Dpl_3qgxNiUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data wrangling is the process of removing errors and combining complex data sets to make them more accessible and easier to analyze. Due to the rapid expansion of the amount of data and data sources available today, storing and organizing large quantities of data for analysis is becoming increasingly necessary."
      ],
      "metadata": {
        "id": "XUXmDVyeRFU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "dataset.head()"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "dataset['Date'] = pd.to_datetime(dataset['Date'])\n",
        "\n",
        "dataset['year'] = dataset['Date'].dt.year\n",
        "dataset['month'] = dataset['Date'].dt.month\n",
        "dataset['day'] = dataset['Date'].dt.day_name()\n",
        "\n",
        "# We dont want each day name so we converted it into binary class as Weekdays = 0 & Weekend 1.\n",
        "\n",
        "dataset['weekdays_weekend']=dataset['day'].apply(lambda x : 1 if x=='Saturday' or x=='Sunday' else 0 )\n",
        "\n",
        "# Droping unnecessary columns.\n",
        "# Year basically contains details from 2017 december to 2018 november so we considers this is one year.\n",
        "dataset=dataset.drop(columns=['Date','day','year'],axis=1)"
      ],
      "metadata": {
        "id": "tnVW_1KLRfRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.columns"
      ],
      "metadata": {
        "id": "bbEQ3AKZRibj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Numeric Features\n",
        "\n",
        "numeric_features= dataset.select_dtypes(exclude='object')\n",
        "numeric_features"
      ],
      "metadata": {
        "id": "ToYmEzdQRqKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer : Here we have deleted Data column which contain data/Month/Year from the dataset and add a new column that is Month to make easy to find the insights."
      ],
      "metadata": {
        "id": "FwvUU1ZJR7xJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1 Mean Skew of Dataset"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "#plotting histogram\n",
        "\n",
        "for col in numeric_features[:]:\n",
        "  sns.histplot(dataset[col])\n",
        "  plt.axvline(dataset[col].mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
        "  plt.axvline(dataset[col].median(), color='cyan', linestyle='dashed', linewidth=2)   \n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have pick this histogrtam chart to find the count the number of the data and mean of that data column to analyse."
      ],
      "metadata": {
        "id": "FOri-_TyWB6T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The features which are skewed, their mean and the median are also skewed."
      ],
      "metadata": {
        "id": "5hUITGM8WI1w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "# ploting Regression plot of each columns of dataset v/s rented bike count columns\n",
        "\n",
        "for col in numeric_features[:]:\n",
        "  if col == 'Rented Bike Count':\n",
        "    pass\n",
        "  else:\n",
        "    sns.regplot(x=dataset[col],y=dataset[\"Rented Bike Count\"],line_kws={\"color\": \"red\"})\n",
        "  \n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer : We have pick up this chart to find the relationship between dependent and independent variable and find the best fit line."
      ],
      "metadata": {
        "id": "LKPV1jClXWGZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer : From this chart we can say that the line drawn shows the relationship between dependent and independent variable in this column some of the independent variable are directly proportional, inversely proportional and some are neither directly nor inversely.\n",
        "\n",
        "Directly Proportiooinal variable are :-\n",
        "\n",
        "Hour\n",
        "\n",
        "Temperature(°C)\n",
        "\n",
        "Wind speed\n",
        "\n",
        "Dew point temperature\n",
        "\n",
        "Solar Radiation\n",
        "\n",
        "Inversely Proportional variable are\n",
        "\n",
        "Humidity\n",
        "\n",
        "Rainfall\n",
        "\n",
        "Snowfall\n",
        "\n",
        "Neither Directly nor Inversely Proportional\n",
        "\n",
        "Month\n",
        "\n",
        "weekdays_weekend\n",
        "\n",
        "Visibility"
      ],
      "metadata": {
        "id": "umwkVP36Xdfy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The columns 'Hour','Temperature', 'Wind_speed','Visibility', and 'Solar_Radiation' are positively related to the dependent variable. Which means that the rented bike count increases with increase of these features.\n",
        "Whereas, the colums 'Rainfall','Snowfall','Humidity' are those features which are negatively related with the dependent variable, which implies that the rented bike count decreases when these features increases."
      ],
      "metadata": {
        "id": "eZ2BlF_TX3OJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3 Season Vs Ranted Bike Count"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "plt.figure(figsize=(20,15),dpi=200)\n",
        "sns.catplot(x ='Seasons', y = 'Rented Bike Count' , data = dataset)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "plt.figure(figsize=(8,8))\n",
        "sns.barplot(x ='Seasons', y = 'Rented Bike Count' , data = dataset)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqwfSM-TY0I0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer : We have pick categorical chart and bar chart to analyse the number of bike which is ranted from which helps to analyse that the given data which is divided into four season we can easily see from the data."
      ],
      "metadata": {
        "id": "Yg1UM654n6LB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer : We have found that average of the ranted bike diveded into four season like Winter, Spring, Summer, Autumn which show that the demond of bike is high in Summmer and lowest in Winter compare to the other."
      ],
      "metadata": {
        "id": "fq1y1jXToCD9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer : Yes we have gain the importent insights that help us to create positive business impect as we can see from the graph that demand of bike is high in Summer after that Autumn after that in Spring and very less in Winter so from the business prespective we can say that the business will give high profit in Summer, Autumn, Spring and less profit in Winter.\n",
        "\n",
        "These shows that high growth in the season like- Summer, Autumn, Sring as compare to winter."
      ],
      "metadata": {
        "id": "BzNQlP7BoT3A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4 Analysis Rented bike demond in different in every month."
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "plt.figure(figsize=(20,15), dpi =200)\n",
        "sns.catplot(x='month', y='Rented Bike Count', data=dataset)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "plt.figure(figsize=(8,8))\n",
        "sns.barplot(x='month', y='Rented Bike Count', data=dataset)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IasY28nopjnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer : We have pick categorical chart and bar chart to analyse the number of bike which is ranted from which helps to analyse that the given data which is divided into 12 month which we can easily see from the data."
      ],
      "metadata": {
        "id": "ANdD6ccsp3zu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer : We have found that average of the ranted bike diveded into 122 month of an year which show that the demond of bike is different in different month like the season wise."
      ],
      "metadata": {
        "id": "hNSUPOCgqCH3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer : Yes we have gain the importent insights that help us to create positive business impect as we can see from the graph that demand of bike is on the basis of month ie demond of bike is changes in everey month so from the business prespective.\n",
        "\n",
        "We can analyse that high demond in the month like - May, June and July and demond is moderate in the month of - March, April, August, September, October, november and low demond in the month - January, February, December.\n",
        "\n",
        "So the company have to ready with the plan on the basis of the month."
      ],
      "metadata": {
        "id": "eIiFOQxGqQlr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5 Weekdays Weekend Data Analysis"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "# Dependant Column Value Counts\n",
        "print(dataset.weekdays_weekend.value_counts())\n",
        "print(\" \")\n",
        "# Dependant Variable Column Visualization\n",
        "dataset['weekdays_weekend'].value_counts().plot(kind='pie',\n",
        "                              figsize=(15,6),\n",
        "                               autopct=\"%1.1f%%\",\n",
        "                               startangle=90,\n",
        "                               shadow=True,\n",
        "                               labels=['Not weekend Days(%)','weekend Days(%)'],\n",
        "                               colors=['purple','blue'],\n",
        "                               explode=[0,0]\n",
        "                              )"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer : We have pick up the pie chart expresses a part-to-whole relationship in your data. It's easy to explain the percentage comparison through area covered in a circle with different colors. Where differenet percentage comparison comes into action pie chart is used frequently. So, I used Pie chart and which helped me to get the percentage comparision of the dependant variable"
      ],
      "metadata": {
        "id": "HgRCAJDiqzME"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer : From the above chart I got to know that, there are 6216 Bike has rented in which is not the weekend days which is 71% of the whole rated bike count data given in the dataset. In other hand, 2544 customers are Bike has rented in which is in weekend days which is 29% of the whole Ranted Bike count data given in the dataset.\n",
        "\n",
        "These shows that demond of bike is high during non Weekend_day as compare to weekend days."
      ],
      "metadata": {
        "id": "u90SKbkfq70t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer : We can analyse that there is high demond in non weekdays_weekend day but if we talk about weekdays_weekend there is no demond so this insight is very important for the business prespective there is high growth in business during non weekdays_weekend and there is less growth in weekdays_weekend."
      ],
      "metadata": {
        "id": "7fSWCjNVrHgd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6 Demond in Functioning Day and Non Functional day"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "# Dependant Column Value Counts\n",
        "print(dataset['Functioning Day'].value_counts())\n",
        "# Dependant Variable Column Visualization\n",
        "dataset['Functioning Day'].value_counts().plot(kind='pie',\n",
        "                              figsize=(15,6),\n",
        "                               autopct=\"%1.1f%%\",\n",
        "                               startangle=90,\n",
        "                               shadow=True,\n",
        "                               labels=['Functional Day(%)','Not Functional Day(%)'],\n",
        "                               colors=['purple','green'],\n",
        "                               explode=[0,0]\n",
        "                              )"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer : We have pick up the pie chart expresses a part-to-whole relationship in your data. It's easy to explain the percentage comparison through area covered in a circle with different colors. Where differenet percentage comparison comes into action pie chart is used frequently. So, I used Pie chart and which helped me to get the percentage comparision of the dependant variable."
      ],
      "metadata": {
        "id": "yU4hZn-Urqvn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer : From the above chart I got to know that, there are 6216 Bike has rented in which is not the weekend days which is 71% of the whole rated bike count data given in the dataset. In other hand, 2544 customers are Bike has rented in which is in weekend days which is 29% of the whole Ranted Bike count data given in the dataset.\n",
        "\n",
        "These shows that demond of bike is high during non Weekend_day as compare to weekend days."
      ],
      "metadata": {
        "id": "c95MCT2xr7bU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer : We can analyse that there is high demond in non weekdays_weekend day but if we talk about weekdays_weekend there is no demond so this insight is very important for the business prespective there is high growth in business during non weekdays_weekend and there is less growth in weekdays_weekend."
      ],
      "metadata": {
        "id": "Y95TAhZtsJrk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7 Demond during Non Holidays and Holiday"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "# Dependant Column Value Counts\n",
        "print(dataset['Holiday'].value_counts())\n",
        "# Dependant Variable Column Visualization\n",
        "dataset['Holiday'].value_counts().plot(kind='pie',\n",
        "                              figsize=(15,6),\n",
        "                               autopct=\"%1.1f%%\",\n",
        "                               startangle=90,\n",
        "                               shadow=True,\n",
        "                               labels=['No Holiday(%)','Holiday(%)'],\n",
        "                               colors=['purple','green'],\n",
        "                               explode=[0,0]\n",
        "                              )"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer : We have pick up the pie chart expresses a part-to-whole relationship in your data. It's easy to explain the percentage comparison through area covered in a circle with different colors. Where differenet percentage comparison comes into action pie chart is used frequently. So, I used Pie chart and which helped me to get the percentage comparision of the dependant variable."
      ],
      "metadata": {
        "id": "yY6sGstltOyV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer : Answer : From the above chart I got to know that, there are 8328 is on No Holiday which is 95.1% of the whole rated bike count data given in the dataset. In other hand, 432 customers are in Holiday days which is 4.9% of the whole Ranted Bike count data given in the dataset."
      ],
      "metadata": {
        "id": "g1T3pj3ItZaK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ansawer : We can analyse that there is high demond in Non Holiday but if we talk about holiday there is no demond so this insight is very important for the business prespective there is high growth in business during non holiday and there is highly negative growth in holidays."
      ],
      "metadata": {
        "id": "4Eav7_RAtn_3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8 Average Bikes Rented per Hour"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "avg_rent_hrs = dataset.groupby('Hour')['Rented Bike Count'].mean()\n",
        "\n",
        "# plot average rent over time(hrs)\n",
        "plt.figure(figsize=(20,4))\n",
        "a=avg_rent_hrs.plot(legend=True,marker='o',title=\"Average Bikes Rented Per Hr\")\n",
        "a.set_xticks(range(len(avg_rent_hrs)));\n",
        "a.set_xticklabels(avg_rent_hrs.index.tolist(), rotation=85);"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart  - 8 visualization code \n",
        "plt.figure(figsize=(20,15),dpi=200)\n",
        "sns.catplot(x='Hour',y='Rented Bike Count',data=dataset)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AWd7trIfuUl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have pick up the pie chart expresses a part-to-whole relationship in your data. It's easy to explain the percentage comparison through area covered in a circle with different colors. Where differenet percentage comparison comes into action pie chart is used frequently. So, I used Pie chart and which helped me to get the percentage comparision of the dependant variable."
      ],
      "metadata": {
        "id": "34tq0bkWuxij"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer : We have pick up the line and cat plot expresses a whole relationship in your data. Line represents the demond continuously."
      ],
      "metadata": {
        "id": "0I4YQkCcu2Hb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer : We can analyse that there is High rise of Rented Bikes from 8:00 a.m to 9:00 p.m means people prefer rented bike during rush hour for the office hour.\n",
        "\n",
        "We can analyse that there is high demond in Non Holiday but if we talk about holiday there is no demond so this insight is very important for the business prespective there is high growth in business during non holiday and there is highly negative growth in holidays."
      ],
      "metadata": {
        "id": "L1eYbKslvBUV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer : We can analyse that there is high demond of Bike during ofice going and coming hours that means during office hours high positive business impact."
      ],
      "metadata": {
        "id": "7W-yhN6ivO6y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9 Demond of bike during Rainfall"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "dataset.columns\n"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.catplot(x='Rainfall(mm)', y='Rented Bike Count',data=dataset)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GaP275T1v3cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.barplot(x='Rainfall(mm)', y='Rented Bike Count',data=dataset)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aMZg-kWcwIoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer : We have pick categorical chart and bar chart to find relationship between ranted bike count and Rainfall by the helps that to analyse the given data."
      ],
      "metadata": {
        "id": "Gux404iwwXQp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer : we can see that if Rainfall increase demand of Rented Bike Decreases."
      ],
      "metadata": {
        "id": "RxFZ16b2wcEm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer : From the given graph we can say that as Rainfall increase demand of Rented Bike Decreases which lead to the negative impect on business."
      ],
      "metadata": {
        "id": "a1Kj0G31wkzm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10  Demond of bike during Snowfall"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "plt.figure(figsize=(8,8))\n",
        "sns.catplot(x='Snowfall (cm)',y='Rented Bike Count',data=dataset)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "plt.figure(figsize=(8,8))\n",
        "sns.barplot(x='Snowfall (cm)',y='Rented Bike Count',data=dataset)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Nat2JJQhxNBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer : We have pick categorical chart and bar chart to find relationship between ranted bike count and Snowfall by the helps that to analyse the given data."
      ],
      "metadata": {
        "id": "wvy_1oCcxY3F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer : we can see that if if there is snowfall then there is decrease in the demond on rented bike."
      ],
      "metadata": {
        "id": "ncr0RpnOxdnu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer : From the given graph we can say that if there is snowfall then there is decrease in demand of Rented Bike Decreases which lead to the negative impect on business."
      ],
      "metadata": {
        "id": "3D5Jeu95xnPx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11 Effect in Rented Bike with respect to temperature"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "plt.figure(figsize=(8,8))\n",
        "sns.catplot(x='Temperature(°C)',y='Rented Bike Count',data=dataset)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "plt.figure(figsize=(8,8))\n",
        "sns.lineplot(x='Temperature(°C)',y='Rented Bike Count',data=dataset)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1kjtWJWWyU2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer : We have pick categorical chart and line chart to find relationship between ranted bike count and Snowfall by the helps that to analyse the given data."
      ],
      "metadata": {
        "id": "Nw_5-K3AyksV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer : From the given graph we say that the rented bike count is directly proportional to temperature as the temperature is high the demond is high."
      ],
      "metadata": {
        "id": "51Kuzt9ZyrHW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "plt.figure(figsize=(15,10))\n",
        "sns.heatmap(dataset.corr(), cmap ='PiYG', annot = True)"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we have seen from the heatmap that temperature and Due point temperature are highly corelated so we can drop one."
      ],
      "metadata": {
        "id": "CLvYwXAZzSaV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer. We have pick up this heatmap chart to find insights to analyse that how the given one variable are the corelation to another variable."
      ],
      "metadata": {
        "id": "mUkc3QhZzVYY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer : We have found that temperature and due point temperature are highly corelated to each orther that why we have to remove any one of them and we can say that temperature and hour is highly effect to dependent variable 'Rented Bike count'."
      ],
      "metadata": {
        "id": "Nesy-WFpzeh2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.columns"
      ],
      "metadata": {
        "id": "LA7TJLQPztWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Droping highly correlated features for eleminating Multico-linearity\n",
        "dataset=dataset.drop(['Dew point temperature(°C)'],axis=1)"
      ],
      "metadata": {
        "id": "_U48sO3Xzzo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13 pair plot"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.head()"
      ],
      "metadata": {
        "id": "AcwVRE-Y0bxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pair plot visulization code\n",
        "sns.pairplot(dataset)"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer : Pair plot is used to understand the best set of features to explain a relationship between two variables or to form the most separated clusters. It also helps to form some simple classification models by drawing some simple lines or make linear separation in our data-set.\n",
        "\n",
        "Thus, I used pair plot to analyse the patterns of data and realationship between the features. It's exactly same as the correlation map but here you will get the graphical representation."
      ],
      "metadata": {
        "id": "7mHLxvV72_zI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer : From the above chart I got to know, there are less linear relationship between variables and data points aren't linearly separable."
      ],
      "metadata": {
        "id": "af4B8d_L3aMQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Hypothesis Testing"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing.\n",
        "\n"
      ],
      "metadata": {
        "id": "fX--JouEH1Wq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have different statistical tests for different scenarios:\n",
        "\n",
        "Single categorical feature -> One proportion test\n",
        "Two categorical features -> Chi squared test\n",
        "More than two category in categorical features -> ANOVA test\n",
        "One numerical and one categorical(=2 categories) feature-> ANOVA test\n",
        "One numerical feature -> T-test\n",
        "Two numerical feature -> Corelation test\n",
        "One numerical and one categorical(>2 categories) feature -> T-test\n",
        "Let's just define three hypothetical statements and perform the needed tests for the same"
      ],
      "metadata": {
        "id": "RgcSMkw4INJj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Statement 1\n",
        "\n",
        "* Null Hypothesis: There is no relation between temperature and rented bike count.\n",
        "* Alternate Hypothesis: There is a relationship between temperature and rented bike count.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "H1tC4-guJKiW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Statement 2:\n",
        "\n",
        "* Null Hypothesis: There is no relationship between holyday and rented bike count.\n",
        "* Alternate Hypothesis: There is a relationship between Holyday and rented bike count.\n",
        "#Statement 3:\n",
        "\n",
        "* Nll Hypothesis: There is no relation between wind speed and rented bike count.\n",
        "* Alternate Hypothesis: There is a relationship between wind speed and rented bike count.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UJ0CFBDFM6mm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "9nbF1VIvRCgk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "wXTdJXnwRdqy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* Null Hypothesis : There is no relation between \"Temperature\" and \"Ranted Bike Coount\"\n",
        "* Alternate Hypothesis : There is a relationship between \"Temperature\" and \"Ranted Bike Count\"\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dF1NMFFbRrm8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Perform an appropriate statistical test"
      ],
      "metadata": {
        "id": "3E9iBAWBTAbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "\n",
        "first_sample1 = dataset[\"Temperature(°C)\"].head(100)\n",
        "second_sample1 = dataset[\"Rented Bike Count\"].head(100)\n",
        "\n",
        "stat, p = pearsonr(first_sample1, second_sample1)\n",
        "print('stat=%.3f, p = %.5f'%(stat, p))\n",
        "if p> 0.05:\n",
        "  print('Accept Null Hypothesis')\n",
        "else:\n",
        "  print('Rejected Null Hypothesis')"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The above statistical test states that 'Rented Bike count' depends on 'Temperature' that is temperature is correlated with Rented Bike count.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gd935yV4UT2f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "lMQZRLWFWaf1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer : Pearson Correlation"
      ],
      "metadata": {
        "id": "Gh21Z0arXTiw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer : To find the relationship between the testing series."
      ],
      "metadata": {
        "id": "-9Zut9i8X8wy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "GZ3h2GNgYHki"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "AfcPhwvGYYB0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "* Null Hypothesis : There is no relation between \"Holiday\" and \"Ranted Bike Coount\"\n",
        "* Alternate Hypothesis : There is a relationship between \"Holiday\" and \"Ranted Bike Coount\"\n",
        "\n"
      ],
      "metadata": {
        "id": "GgartF63Y_6U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Perform an appropriate statistical test"
      ],
      "metadata": {
        "id": "EBWzGn2sZuQF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "first_sample2 = dataset['Holiday'].head(100)\n",
        "second_sample2 = dataset[\"Rented Bike Count\"].head(100)\n",
        "\n",
        "stat, p = spearmanr(first_sample2, second_sample2)\n",
        "print('stat=%.3f, p = %.2f'%(stat, p))\n",
        "if p> 0.05:\n",
        "  print('Accept Null Hypothesis')\n",
        "else:\n",
        "  print('Rejected Null Hypothesis')"
      ],
      "metadata": {
        "id": "emqzlFYEaGdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "The above statistical test states that 'Rented Bike count' depends on 'Holiday' that is Holiday is correlated with Rented Bike count."
      ],
      "metadata": {
        "id": "R89zayKsaa-L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Z-dD8e3oa1hz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer : Spearmanr Correlation"
      ],
      "metadata": {
        "id": "Neb2DLrAbKzn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "bG1R26AbbNwO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer : To find the relationship between the testing series."
      ],
      "metadata": {
        "id": "7AD-DwFGbWE5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "KVOspL8IcDc_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis: This is no relation between \"wind speed\" and Rented \"Bike Count\"\n",
        "\n",
        "Alternate Hypothesis: There is relationship between \"wind speed\" and \"Rented Bike Count\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XCOZgAy-dD-0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Perform an appropriate statistical test"
      ],
      "metadata": {
        "id": "yyFF9Qkyi7iL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "from scipy.stats import pearsonr\n",
        "first_sample = dataset[\"Wind speed (m/s)\"].head(100)\n",
        "second_sample = dataset[\"Rented Bike Count\"].head(100)\n",
        "\n",
        "stat, p = pearsonr(first_sample, second_sample)\n",
        "print('stat=%.3f, p = %.2f'%(stat, p))\n",
        "if p> 0.05:\n",
        "  print('Accept Null Hypothesis')\n",
        "else:\n",
        "  print('Rejected Null Hypothesis')"
      ],
      "metadata": {
        "id": "zgQePcQKjLTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####The above statistical test states that 'Rented Bike count' does not depends on 'Wind Speed' that is Wind Speed is not is correlated with Rented Bike count."
      ],
      "metadata": {
        "id": "Sfy4UTpxjYfQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "QohiJE7wkC5W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer : Pearsonr Correlation"
      ],
      "metadata": {
        "id": "WoA8YWRhktKM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "qzhXqwjck0KH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer : To find the relationship between the testing series."
      ],
      "metadata": {
        "id": "wF3UeEqnk4HY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. Feature Engineering & Data Pre-processing"
      ],
      "metadata": {
        "id": "3960ivMBlJY8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "ysyvADBllXo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "dataset.isna().sum()"
      ],
      "metadata": {
        "id": "_OaLBU_kljV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "As we can see that there is no null value persent into it therefore there is not a reguirement to handle missing value and null value of the data."
      ],
      "metadata": {
        "id": "KjmWLCUylwO-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "KCdDL25hl75-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here. When we are working with large set of data then there is chance that missing value persent into it so we have to handle error and following are the techenique to handle missing value--\n",
        "\n",
        "\n",
        "1. Deleting Rows with missing values.\n",
        "2. Impute missing values for continuous variable.\n",
        "\n",
        "3. Using Algorithms that support missing values.\n",
        "4. Prediction of missing values.\n",
        "\n",
        "5. Imputation using Deep Learning Library.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "N2xH_miemOt-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Handling Outliers"
      ],
      "metadata": {
        "id": "wOQeUdWfnf6G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "YfFCHnnhnuQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.info()"
      ],
      "metadata": {
        "id": "IUwpHEo5n1qR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define variable\n",
        "continuous_variable = ['Wind speed (m/s)','Solar Radiation (MJ/m2)','Rainfall(mm)','Temperature(°C)','Visibility (10m)','Humidity(%)','Hour','Snowfall (cm)','Rented Bike Count']\n",
        "categorical_variable =['Sasons','Holiday','Functioning Day','Weekdays_weekend','Month']\n",
        "object_data =['Seasons','Month','Holiday','Functional Day',]"
      ],
      "metadata": {
        "id": "7dOhBVDFn-Hw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# code to find outliers\n",
        "plt.figure(figsize=(30,15))\n",
        "for n,column in enumerate(dataset.describe().columns):\n",
        "  plt.subplot(5, 4, n+1)\n",
        "  sns.boxplot(dataset[column])\n",
        "  plt.title(f'{column.title()}',weight='bold')\n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "dwUrw9N0oGII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining the code for outlier detection and percentage using IQR.\n",
        "def detect_outliers(data):\n",
        "    outliers = []\n",
        "    data = sorted(data)\n",
        "    q1 = np.percentile(data, 25)\n",
        "    q2 = np.percentile(data, 50)\n",
        "    q3 = np.percentile(data, 75)\n",
        "    print(f\"q1:{q1}, q2:{q2}, q3:{q3}\")\n",
        "\n",
        "    IQR = q3-q1\n",
        "    lwr_bound = q1-(1.5*IQR)\n",
        "    upr_bound = q3+(1.5*IQR)\n",
        "    print(f\"Lower bound: {lwr_bound}, Upper bound: {upr_bound}, IQR: {IQR}\")\n",
        "\n",
        "    for i in data: \n",
        "        if (i<lwr_bound or i>upr_bound):\n",
        "            outliers.append(i)\n",
        "    len_outliers= len(outliers)\n",
        "    print(f\"Total number of outliers are: {len_outliers}\")\n",
        "\n",
        "    print(f\"Total percentage of outlier is: {round(len_outliers*100/len(data),2)} %\")"
      ],
      "metadata": {
        "id": "GSYpjB0xoXQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determining IQR, Lower and Upper bound and number out outliers present in each of the continous numerical feature\n",
        "for feature in continuous_variable:\n",
        "  print(feature,\":\")\n",
        "  detect_outliers(dataset[feature])\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "id": "yQjstUXnodZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below mentioned continous features with the percentage of outliers:\n",
        "\n",
        "\n",
        "1. \"Wind Sped\" - 1.84%\n",
        "2. \"Solar Radiation\" - 7.32%\n",
        "\n",
        "3. \"Rainfall\" - 6.03%\n",
        "4. \"Snowfall\" - 5.06%\n",
        "\n",
        "5. \"Rented Bike Count\" - 1.8%\n",
        "\n",
        "Let's define a function for the outlier treatment using IQR technique and cap the outliers in 25-75 percentile.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2bXP0V6YonVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the function that treats outliers with the IQR technique\n",
        "def treat_outliers_iqr(data):\n",
        "    # Calculate the first and third quartiles\n",
        "    q1, q3 = np.percentile(data, [25, 75])\n",
        "    \n",
        "    # Calculate the interquartile range (IQR)\n",
        "    iqr = q3 - q1\n",
        "    \n",
        "    # Identify the outliers\n",
        "    lower_bound = q1 - (1.5 * iqr)\n",
        "    upper_bound = q3 + (1.5 * iqr)\n",
        "    outliers = [x for x in data if x < lower_bound or x > upper_bound]\n",
        "    \n",
        "    # Treat the outliers (e.g., replace with the nearest quartile value)\n",
        "    treated_data = [q1 if x < lower_bound else q3 if x > upper_bound else x for x in data]\n",
        "    treated_data_int = [int(absolute) for absolute in treated_data]\n",
        "    \n",
        "    return treated_data_int"
      ],
      "metadata": {
        "id": "kQ4yWKFLqNTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Passing all the feature one by one from the list of continous_value_feature in our above defined function for outlier treatment\n",
        "for feature in continuous_variable:\n",
        "  dataset[feature]= treat_outliers_iqr(dataset[feature])"
      ],
      "metadata": {
        "id": "fNFQYCfFqYiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(30,15))\n",
        "for n,column in enumerate(dataset.describe().columns):\n",
        "  plt.subplot(5, 4, n+1)\n",
        "  sns.boxplot(dataset[column])\n",
        "  plt.title(f'{column.title()}',weight='bold')\n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "TZt5TRx6qkUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rechecking the total number of outliers and its percentage present in our dataset.\n",
        "for feature in continuous_variable:\n",
        "  print(feature,\":\")\n",
        "  detect_outliers(dataset[feature])\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "id": "5HwQiuTurZh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bivariate analysis of Outliers"
      ],
      "metadata": {
        "id": "Ym67C2jdrh4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_variable =['Seasons','Holiday','Functioning Day']"
      ],
      "metadata": {
        "id": "68KBXicWrp4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Checking the outliers present in each category\n",
        "plt.figure(figsize=(18,14))\n",
        "for i,j in enumerate(categorical_variable):\n",
        "  plt.subplot(2,2,i+1)\n",
        "  sns.boxplot(x=dataset[j], y=dataset[\"Rented Bike Count\"])\n",
        "  plt.title(f\"Box plot for {j} feature\")"
      ],
      "metadata": {
        "id": "oPXw9P4DrxP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determining IQR, Lower and Upper bound and number out outliers present in each of the category of object dtype features\n",
        "for feature in categorical_variable:\n",
        "  print(f\"Feature: {feature}\")\n",
        "  for num,cat in enumerate(dataset[feature].unique().tolist()):\n",
        "    print(f\"{num+1}: Category: {cat}\")\n",
        "    detect_outliers(dataset[dataset[feature]==cat][\"Rented Bike Count\"])\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "DxfC65fDr-cX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although we have some categorical outliers in the dataset but we will not treat them because we are going to implement ML model and algorithm can easily handle these categorical outliers without information loss."
      ],
      "metadata": {
        "id": "8lNxSZapsMDY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "ryT1A-5RsYAb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since, the outliers present in some of the continous features i.e \"Wind Speed\", \"Solar Radiation\", \"Rainfall\", \"Snowfall\", \"Rentedee Bike Count\" having the percentage 1.84%, 7.32% ,6.03%, 5.06%, 1.8% respectively.\n",
        "\n",
        "We have defined the two seperate funtions one is for \"outlier detection\" and the other is for \"outlier treatment using IQR\" and passed all the observations of continous features through it. We have successfully capped out extreme left outliers(<25%) and extreme outliers (>75%) in the 25th and 75th quartile value."
      ],
      "metadata": {
        "id": "ccgPsvWlshxu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "asBnVx09srA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Extracting categorical features\n",
        "categorical_features= dataset.select_dtypes(include='object')\n",
        "categorical_features"
      ],
      "metadata": {
        "id": "z0BHBPxZs1TQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns of Season \n",
        "dataset['Winter'] = np.where(dataset['Seasons']=='Winter',1,0)\n",
        "dataset['Spring'] = np.where(dataset['Seasons']=='Spring',1,0)\n",
        "dataset['Summer'] = np.where(dataset['Seasons']=='Summer',1,0)\n",
        "dataset['Autumn'] = np.where(dataset['Seasons']=='Autumn',1,0)\n",
        "\n",
        "# Drop the original column Season from the dataframe\n",
        "dataset.drop(columns=['Seasons'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "WR_koa-Cs_rn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find the all unique categorical data  of Holiday\n",
        "dataset['Holiday'].unique()"
      ],
      "metadata": {
        "id": "tc5A5daGtGzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns of Holiday\n",
        "dataset[' Holiday '] = np.where(dataset['Holiday']=='Holiday' ,1,0)\n",
        "dataset['No Holiday'] = np.where(dataset['Holiday']==' No Holiday' ,1,0)\n",
        "\n",
        "\n",
        "# Drop the original column Holiday from the dataframe\n",
        "dataset.drop(columns=['Holiday'],axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "FbPs2pu4tQsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['Functioning Day'].unique()"
      ],
      "metadata": {
        "id": "y2fydPNltXi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns of Holiday\n",
        "dataset['Functional day'] = np.where(dataset['Functioning Day']=='Yes',1,0)\n",
        "dataset['Not Functional day'] = np.where(dataset['Functioning Day']=='No',1,0)\n",
        "\n",
        "# Drop the original column Holiday from the dataframe\n",
        "dataset.drop(columns=['Functioning Day'],axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "XDNZiHKzte9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.head()"
      ],
      "metadata": {
        "id": "a48y9t3Jtjuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns of Hour, Month, Weekend column\n",
        "cols=['Hour','month','weekdays_weekend']\n",
        "for col in cols:\n",
        "  dataset[col]=dataset[col].astype('category')"
      ],
      "metadata": {
        "id": "05VLf4J_tqws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Pandas get Dummies for Encoding categorical features\n",
        "dataset = pd.get_dummies(dataset,drop_first=True,sparse=True)\n",
        "dataset.head()"
      ],
      "metadata": {
        "id": "uRj5kvVPtt4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.columns"
      ],
      "metadata": {
        "id": "LXxXRpnmt7I_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.shape\n"
      ],
      "metadata": {
        "id": "yNfvSTDPt-oY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "HT5WlUOiuPxP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer :\n",
        "\n",
        "a. We have used one-hot encoding technique to change our categorical features of object type into int type by creating their dummies so that it becomes compatible to feed it into various ML algorithms in future.\n",
        "\n",
        "b. Since, we have 3 to 4 unique orderless categories in all the categorical features (which is less in number). So, it is good to use Nominal encoding technique than ordinal."
      ],
      "metadata": {
        "id": "qFJI4Tanuatp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Normalization"
      ],
      "metadata": {
        "id": "nc9BS6lswW3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Distribution plot of Rented Bike Count\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.distplot(dataset['Rented Bike Count'])"
      ],
      "metadata": {
        "id": "QNA4qfAxxIMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Applying square root to Rented Bike Count to improve skewness\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.distplot(np.sqrt(dataset['Rented Bike Count']))"
      ],
      "metadata": {
        "id": "_zrvIM5GxbJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Transformation"
      ],
      "metadata": {
        "id": "it-yKOszxvxz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "MEw-U4Esx3LI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.columns"
      ],
      "metadata": {
        "id": "mtzsx9kByEmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "dataset['Rented Bike Count']=np.log1p(dataset['Rented Bike Count'])"
      ],
      "metadata": {
        "id": "qFtWcaqYyOOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Data Scaling"
      ],
      "metadata": {
        "id": "peNVargByWiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "X = dataset.drop(columns = ['Rented Bike Count'] , axis = 1)\n",
        "y = dataset['Rented Bike Count']"
      ],
      "metadata": {
        "id": "81t57Ti6yghq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Data Splitting"
      ],
      "metadata": {
        "id": "vN1llxfRytHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "X_train , X_test, y_train, y_test =train_test_split(X, y, test_size= .2 , random_state =0 )\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "y_train.shape"
      ],
      "metadata": {
        "id": "XQ6ji9c6y-kN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "2FgV020AzIDP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer : we have taken 80% for Training Data and 20% for test data because we want to go by the standard norms distribution."
      ],
      "metadata": {
        "id": "L5bzk4RHzQB8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  ML Model Implementation"
      ],
      "metadata": {
        "id": "X9Tnyfad2w1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing essential libraries to check the accuracy\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_absolute_percentage_error "
      ],
      "metadata": {
        "id": "0gRnoNh629P7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the function that calculated regression metrics\n",
        "def regression_metrics(y_train_actual,y_train_pred,y_test_actual,y_test_pred):\n",
        "  print(\"-\"*50)\n",
        "  ## mean_absolute_error\n",
        "  MAE_train= mean_absolute_error(y_train,y_train_pred)\n",
        "  print(\"MAE on train is:\" ,MAE_train)\n",
        "  MAE_test= mean_absolute_error(y_test,y_test_pred)\n",
        "  print(\"MAE on test is:\" ,MAE_test)\n",
        "\n",
        "  print(\"-\"*50)\n",
        "\n",
        "  ## mean_squared_error\n",
        "  MSE_train= mean_squared_error(y_train, y_train_pred)\n",
        "  print(\"MSE on train is:\" ,MSE_train)\n",
        "  MSE_test  = mean_squared_error(y_test, y_test_pred)\n",
        "  print(\"MSE on test is:\" ,MSE_test)\n",
        "\n",
        "  print(\"-\"*50)\n",
        "\n",
        "  ## root_mean_squared_error\n",
        "  RMSE_train = np.sqrt(MSE_train)\n",
        "  print(\"RMSE on train is:\" ,RMSE_train)\n",
        "  RMSE_test = np.sqrt(MSE_test)\n",
        "  print(\"RMSE on test is:\" ,RMSE_test)\n",
        "\n",
        "  print(\"-\"*50)\n",
        "\n",
        "  ## root_mean_squared_error\n",
        "  RMSE_train = np.sqrt(MSE_train)\n",
        "  print(\"RMSE on train is:\" ,RMSE_train)\n",
        "  RMSE_test = np.sqrt(MSE_test)\n",
        "  print(\"RMSE on test is:\" ,RMSE_test)\n",
        "\n",
        "  print(\"-\"*50)\n",
        "\n",
        "  ## mean_absolute_percentage_error\n",
        "  MAPE_train = mean_absolute_percentage_error(y_train, y_train_pred)*100\n",
        "  print(\"MAPE on train is:\" ,MAPE_train, \" %\")\n",
        "  MAPE_test = mean_absolute_percentage_error(y_test, y_test_pred)*100\n",
        "  print(\"MAPE on test is:\" ,MAPE_test, \" %\")\n",
        "\n",
        "  print(\"-\"*50)\n",
        "\n",
        "  ## r2_score\n",
        "  R2_train= r2_score(y_train,y_train_pred)\n",
        "  print(\"R2 on train is:\" ,R2_train)  \n",
        "  R2_test= r2_score(y_test,y_test_pred)\n",
        "  print(\"R2 on test is:\" ,R2_test)\n",
        "\n",
        "  print(\"-\"*50)\n",
        "\n",
        "   ## Adjusted R2_score\n",
        "  Adj_R2 = (1-(1-r2_score(y_train, y_train_pred))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))\n",
        "  print( 'Adjusted R2 on train is :', Adj_R2)\n",
        "  Adj_R2 = (1-(1-r2_score(y_test, y_test_pred))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))\n",
        "  print( 'Adjusted R2 on test is :', Adj_R2)\n",
        "\n",
        "  print(\"-\"*50)"
      ],
      "metadata": {
        "id": "FjPo_hHZ3Iy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "98IaAxbg4FaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Regression"
      ],
      "metadata": {
        "id": "68yTPG2u4NxS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing LinearRegression from sklearn\n",
        "from sklearn.linear_model import LinearRegression"
      ],
      "metadata": {
        "id": "260aaFY24Z4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "regressor= LinearRegression()\n",
        "\n",
        "# Fit the Algorithm\n",
        "regressor.fit(X_train,y_train)\n",
        "\n",
        "# Predict the model\n",
        "y_train_regression_pred= regressor.predict(X_train)\n",
        "y_test_regression_pred= regressor.predict(X_test)"
      ],
      "metadata": {
        "id": "k3ldn5zQ4gI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regressor.score(X_train, y_train)"
      ],
      "metadata": {
        "id": "16I7GlNI5F6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regressor.coef_"
      ],
      "metadata": {
        "id": "fF-KQlIn5jsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regressor.intercept_"
      ],
      "metadata": {
        "id": "Oa5a9khj56Aj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Yk3to3JN6H8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the regression metrics\n",
        "regression_metrics(y_train,y_train_regression_pred,y_test,y_test_regression_pred)"
      ],
      "metadata": {
        "id": "b46sTGyC6QbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting the figure\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.plot(y_test_regression_pred, color='Blue')\n",
        "plt.plot(np.array(y_test), color='Red')\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "25ZNVnIp6aLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have started with the most basic and simple ML model i.e Linear Regression. We have tried to evaluate the most important regression metics on both the train and test datesets so that we can conclude our ML model. Here for Linear Regression, we can observe that both the r2 scores are pretty close which explains that on test dataset and our model is following the correct way.\n",
        "\n",
        "We can comprehend that 'dependent' and 'independent' variables and y we got 0.83 maximum r2 score in LR model implementation.\n",
        "\n",
        "In order to fetch good and more accurate results, we shall go for cross- Validation & Hyperparameter Tuning of 'Lasso', 'Ridge' and 'Elastic Net' models."
      ],
      "metadata": {
        "id": "ciKlEWxI6kvI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "EWmShQBo6tgu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ridge (L2) Regression"
      ],
      "metadata": {
        "id": "L5dzqnXq7BDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import ridge regression from sklearn library\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Creating Ridge instance\n",
        "ridge= Ridge()\n",
        "\n",
        "# Defining parameters\n",
        "parameters = {\"alpha\": [1e-1,1,5,7,10,11,14,15,16,17], \"max_iter\":[1,2,3]}\n",
        "\n",
        "# Train the model\n",
        "ridgeR = GridSearchCV(ridge, parameters, scoring='neg_mean_squared_error', cv=3)\n",
        "ridgeR.fit(X_train,y_train)\n",
        "\n",
        "# Predict the output\n",
        "y_train_ridge_pred = ridgeR.predict(X_train)\n",
        "y_test_ridge_pred = ridgeR.predict(X_test)\n",
        "\n",
        "# Printing the best parameters obtained by GridSearchCV\n",
        "print(f\"The best alpha value found out to be: {ridgeR.best_params_}\")\n",
        "print(f\"Negative mean square error is: {ridgeR.best_score_}\")"
      ],
      "metadata": {
        "id": "-icG5xx_6dgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating regression metrics for Ridge\n",
        "regression_metrics(y_train,y_train_ridge_pred,y_test,y_test_ridge_pred)"
      ],
      "metadata": {
        "id": "uXbmPHmD7hVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lasso (L1) Regression"
      ],
      "metadata": {
        "id": "T4h8AZ_K8K7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import lasso regression from sklearn library\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Creating Ridge instance\n",
        "lasso= Lasso()\n",
        "\n",
        "# Defining parameters\n",
        "parameters_lasso = {\"alpha\": [1e-5,1e-4,1e-3,1e-2,1e-1,1,5], \"max_iter\":[7,8,9,10]}\n",
        "\n",
        "# Train the model\n",
        "lassoR = GridSearchCV(lasso, parameters_lasso, scoring='neg_mean_squared_error', cv=5)\n",
        "lassoR.fit(X_train,y_train)\n",
        "\n",
        "# Predict the output\n",
        "y_train_lasso_pred = lassoR.predict(X_train)\n",
        "y_test_lasso_pred = lassoR.predict(X_test)\n",
        "\n",
        "# Printing the best parameters obtained by GridSearchCV\n",
        "print(f\"The best alpha value found out to be: {lassoR.best_params_}\")\n",
        "print(f\"Negative mean square error is: {lassoR.best_score_}\")"
      ],
      "metadata": {
        "id": "nckoFZYv7ldx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating regression metrics for Ridge\n",
        "regression_metrics(y_train,y_train_lasso_pred,y_test,y_test_lasso_pred)"
      ],
      "metadata": {
        "id": "LO_Mw6l78r9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting the figure\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.plot(y_test_lasso_pred, color='Blue')\n",
        "plt.plot(np.array(y_test), color='Red')\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "z9W9nhZ-82Pz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Elastic Net Regresson"
      ],
      "metadata": {
        "id": "42EqBwfi9lbF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import elastic net regression from sklearn library\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Creating e_net instance\n",
        "e_net= ElasticNet()\n",
        "\n",
        "# Defining hyperparameters\n",
        "parameters_e_net = {\"alpha\": [1e-5,1e-4,1e-3,1e-2,1,5], \"max_iter\":[12,13,14,15]}\n",
        "\n",
        "# Train the model\n",
        "e_netR = GridSearchCV(e_net, parameters_e_net, scoring='neg_mean_squared_error', cv=5)\n",
        "e_netR.fit(X_train,y_train)\n",
        "\n",
        "# Predict the output\n",
        "y_train_e_net_pred = e_netR.predict(X_train)\n",
        "y_test_e_net_pred = e_netR.predict(X_test)\n",
        "\n",
        "# Printing the best parameters obtained by GridSearchCV\n",
        "print(f\"The best alpha value found out to be: {e_netR.best_params_}\")\n",
        "print(f\"Negative mean square error is: {e_netR.best_score_}\")"
      ],
      "metadata": {
        "id": "DSqjIlry9veV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating regression metrics for Elastic Net\n",
        "regression_metrics(y_train,y_train_e_net_pred,y_test,y_test_e_net_pred)"
      ],
      "metadata": {
        "id": "g3AoOZtQ95lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting the figure\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.plot(y_test_e_net_pred, color='Blue')\n",
        "plt.plot(np.array(y_test), color='Red')\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "M34jlk0_-Ij6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "F3hap8qT-SEM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer : We have used GridSearchCV as the hyperparameter optimization technique as it uses all possible combinations of hyperparameters and their values. It then calculates the performance for each combination and selects the best value for the hyperparameters. This offers the most accurate tuning method."
      ],
      "metadata": {
        "id": "aBiKwWBy-gCH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "1yeCB-ak-oji"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer : Despite using Lasso, Ridge and Elastic net models, we couldn't see any significant improvement in the r2 score, MSE and on MAPE as well. This shows that we have to go for higher and more complex ML models like Decision trees, Random Forest, LightGBM Regression and XGBoost Regression."
      ],
      "metadata": {
        "id": "Q87FCuUK-xzr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ML Model - 2 Implementing Decision Tree Regression"
      ],
      "metadata": {
        "id": "fkieadJl-8jc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the regressor\n",
        "from sklearn.tree import DecisionTreeRegressor \n",
        "  \n",
        "# create a regressor object\n",
        "TreeR = DecisionTreeRegressor(max_depth=10) \n",
        "  \n",
        "# fit the regressor with X and Y data\n",
        "TreeR.fit(X_train, y_train)\n",
        "\n",
        "# predict the model\n",
        "y_train_tree_pred= TreeR.predict(X_train)\n",
        "y_test_tree_pred= TreeR.predict(X_test)"
      ],
      "metadata": {
        "id": "hHIITp03_IPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating Regression Metrics\n",
        "regression_metrics(y_train,y_train_tree_pred,y_test,y_test_tree_pred)"
      ],
      "metadata": {
        "id": "lxdR2Bab_LbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "nOk0cB5K_f7C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer After apply LR models, we tried 'Decision Tree' and we saw a good increment in the r2 score from 0.83 to 0.84 that means \"90% Variance of our test dataset is captured by our trained model\" which is excellent. On the other side our RMSE also decreased and shifted below 5(=4.7) which is very good.Also accuracy increased from 93% to 95%. On the other hand from the residual plot our values of mean and median are shifting towards 0 that means our model is improving. But, in the quest of more accurate and real predictions, we decided to further tune the hyperparameters and check the results."
      ],
      "metadata": {
        "id": "6lN1y4vL_pEH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer : After apply Linear Regression model, We tried 'Decision Tree' and we see that r2 score have been increased by 1% that is .83 to .849 that mean '84.9%' Variannce of our thes data has been captured by trained the model which is good so we have decided to further tune the hyperparameters and check the results."
      ],
      "metadata": {
        "id": "kcCzcg-C_z9P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "j1NQDUSj_8tK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decision Tree with GridSearchCV"
      ],
      "metadata": {
        "id": "YBStNGHTAFHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import ridge regression from sklearn library\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Creating Ridge instance\n",
        "decision_tree= DecisionTreeRegressor()\n",
        "\n",
        "# Defining parameters\n",
        "parameters= {'max_depth': [8,9,10], 'min_samples_leaf': [6,7,8], 'min_samples_split': [1,2,4]}\n",
        "\n",
        "# Train the model\n",
        "decision_treeR = GridSearchCV(decision_tree, parameters, scoring='neg_mean_squared_error', cv=3)\n",
        "decision_treeR.fit(X_train,y_train)\n",
        "\n",
        "# Predict the output\n",
        "y_train_grid_Dtree_pred = decision_treeR.predict(X_train)\n",
        "y_test_grid_Dtree_pred = decision_treeR.predict(X_test)\n",
        "\n",
        "# Printing the best parameters obtained by GridSearchCV\n",
        "print(f\"The best alpha value found out to be: {decision_treeR.best_params_}\")\n",
        "print(f\"Negative mean square error is: {decision_treeR.best_score_}\")"
      ],
      "metadata": {
        "id": "-J-X5C6uAgHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating Regression Metrics\n",
        "regression_metrics(y_train,y_train_grid_Dtree_pred,y_test,y_test_grid_Dtree_pred)"
      ],
      "metadata": {
        "id": "KL0w73LFA0C1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting the figure\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.plot(y_test_grid_Dtree_pred, color='Blue')\n",
        "plt.plot(np.array(y_test), color='Red')\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bXUiTWDnA95h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HDiaPx0hBIbZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answwer : We have used GridSearchCV as the hyperparameter optimization technique as it uses all possible combinations of hyperparameters and provides the more accurate results. It then calculates the performance for each combination and selects the best value for the hyperparameters. This offers the most accurate tuning method."
      ],
      "metadata": {
        "id": "m6zMuB4BBVgC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "XlAQWZOmBY0L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used different combinations of parameters to get the best value of r2 score and least MAPE for our case. The best combination was found out to be {'max_depth': [8,9, 10], 'min_samples_leaf':[6, 7, 8] 'min_samples_split':[1, 2, 3, 4} which resulted into the improvement in the MSE from 43% to 34% on the test dataset by hyperparameter tuning of Decision trees."
      ],
      "metadata": {
        "id": "Ja-Qw_0HBjrk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "uYmmc6tUBz50"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to minimise the errors between actual and predicted values, we evaluate our ML model using different metrics. All these metrics try to give us an indication on how close we are with the real/expected output. In our case, each evaluation metric is showing not much difference on the train and test data which shows that our model is predicting a closer expected value. So the Rented Bike Count, the dependent variable, which impacts the business is getting accurately predicted to the extent of ~ 86.9% and ~3% far from the mean of actual absolute values."
      ],
      "metadata": {
        "id": "VIkm42NlB_Ek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ML Model - Implementing Random Forest Regressor"
      ],
      "metadata": {
        "id": "qFYaSgunCJsw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the regressor\n",
        "from sklearn.ensemble import RandomForestRegressor \n",
        "  \n",
        "# create a regressor object\n",
        "RF_TreeR = RandomForestRegressor(n_estimators=100, max_depth=10) \n",
        "  \n",
        "# fit the regressor with X and Y data\n",
        "RF_TreeR.fit(X_train, y_train)\n",
        "\n",
        "# predict the model\n",
        "y_train_RFtree_pred= RF_TreeR.predict(X_train)\n",
        "y_test_RFtree_pred= RF_TreeR.predict(X_test)"
      ],
      "metadata": {
        "id": "gUW_JPc_CSnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating Regression Metrics using RandomForestRegressor\n",
        "regression_metrics(y_train,y_train_RFtree_pred,y_test,y_test_RFtree_pred)"
      ],
      "metadata": {
        "id": "vp_oeeVhCdJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "b_jgYyUWCnry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By implimenting using our third model i.e Random Forest we have achieved the r2 score of 0.91 on training and 0.88 on test dataset that is very good MSE also reduced from 34 to 30 and that means our model is moving towards optimal model.\n",
        "\n",
        "We have increased r2 score (86%) form Decission Tree to r2 score (88%) in Rendom forest."
      ],
      "metadata": {
        "id": "o9_EM3aJCxW9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "TkcXlBHHC66L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Random Forest with RandomizedSearchCV"
      ],
      "metadata": {
        "id": "yzQcA1tdDFS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import ridge regression from sklearn library\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Creating Ridge instance\n",
        "RF_tree= RandomForestRegressor()\n",
        "\n",
        "# Defining parameters\n",
        "parameters= {'n_estimators':[100], 'max_depth': [10,11,12], 'min_samples_leaf': [1, 2]}\n",
        "\n",
        "# Train the model\n",
        "RF_treeR = RandomizedSearchCV(RF_tree, parameters, n_iter=5, n_jobs=-1, scoring='neg_mean_squared_error', cv=3,  verbose=3)\n",
        "RF_treeR.fit(X_train,y_train)\n",
        "\n",
        "# Predict the output\n",
        "y_train_grid_RFtree_pred = RF_treeR.predict(X_train)\n",
        "y_test_grid_RFtree_pred = RF_treeR.predict(X_test)\n",
        "\n",
        "# Printing the best parameters obtained by GridSearchCV\n",
        "print(f\"The best alpha value found out to be: {RF_treeR.best_params_}\")\n",
        "print(f\"Negative mean square error is: {RF_treeR.best_score_}\")"
      ],
      "metadata": {
        "id": "geKqvFO5DOzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating Regression Metrics using GridSearchCV in RandomForestRegressor\n",
        "regression_metrics(y_train,y_train_grid_RFtree_pred,y_test,y_test_grid_RFtree_pred)"
      ],
      "metadata": {
        "id": "0wFuTeIJE_Tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting the figure\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.plot(y_test_grid_RFtree_pred, color='Blue')\n",
        "plt.plot(np.array(y_test), color='Red')\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zm9iScmmFJ7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "jLGGCusSFVTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used RandomizedSearchCV in Random Forest since we have huge dataset and it is good for huge and complex models where we just want to select random parameters from the bag of parameters. It reduces the processing and training time by taking the random subsets of the provided parameters wihout compromising the accuracy of the model."
      ],
      "metadata": {
        "id": "5vCOsRwkFf9q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ktFUb9zNFqan"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After using RandomizedSearchCV with different hyperparameters we saw that their is not much significant improvement observed. Although MSE on test dataset has been reduced from 14 to 13."
      ],
      "metadata": {
        "id": "CYmEjUR5F0KX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ML Model - 4 - LightGBM Regression"
      ],
      "metadata": {
        "id": "Tb7tG0ryGMXq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LightGBM Regression"
      ],
      "metadata": {
        "id": "XPzKAfw2GUyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the regressor\n",
        "from lightgbm import LGBMRegressor\n",
        "  \n",
        "# create a regressor object\n",
        "lgbmR = LGBMRegressor(boosting_type='gbdt', max_depth=4, learning_rate=0.1, n_estimators=500,  n_jobs=-1) \n",
        "  \n",
        "# fit the regressor with X and Y data\n",
        "lgbmR.fit(X_train, y_train)\n",
        "\n",
        "# predict the model\n",
        "y_train_lgbmR_pred= lgbmR.predict(X_train)\n",
        "y_test_lgbmrR_pred= lgbmR.predict(X_test)"
      ],
      "metadata": {
        "id": "3SAIzDVuGf1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating Regression Metrics using RandomForestRegressor\n",
        "regression_metrics(y_train, y_train_lgbmR_pred, y_test, y_test_lgbmrR_pred)"
      ],
      "metadata": {
        "id": "CjJpUfWVGplp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "iPt228bpG0L1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer : LightGBM is the lighter version of GBM. It has more faster and accurate than other popular gradient boosting libraries such as XGBoost on several datasets. We want improved further, so we have tried implimenting LightGBM in order to achieve more accurate results.\n",
        "\n",
        "We saw that with the help of LightGBM we are able to capture 91% of the Variance of the dependent varibale with the help of independent variables(r2 score) for testing dataset.\n",
        "\n",
        "We have further checked the performance metrics by hyperparameter tuning of LightGBM."
      ],
      "metadata": {
        "id": "REIq9Zn-HAnz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "BVUoRrX_HJa7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LightGBM with RandomizedSearchCV"
      ],
      "metadata": {
        "id": "yFS5osYEHReE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import ridge regression from sklearn library and RandomizedSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Creating XGBoost instance\n",
        "lgbm= LGBMRegressor()\n",
        "\n",
        "# Defining parameters\n",
        "parameters={\"learning_rate\":[0.01,0.1],\"max_depth\":[3,4,5],\"n_estimators\":[500,600]}\n",
        "\n",
        "# Train the model\n",
        "lgbm_rand_R= RandomizedSearchCV(lgbm,parameters,scoring='neg_mean_squared_error',n_jobs=-1,cv=3,verbose=3)\n",
        "lgbm_rand_R.fit(X_train,y_train)\n",
        "\n",
        "# Predict the output\n",
        "y_train_rand_lgbm_pred = lgbm_rand_R.predict(X_train)\n",
        "y_test_rand_lgbm_pred = lgbm_rand_R.predict(X_test)\n",
        "\n",
        "# Printing the best parameters obtained by GridSearchCV\n",
        "print(f\"The best alpha value found out to be: {lgbm_rand_R.best_params_}\")\n",
        "print(f\"Negative mean square error is: {lgbm_rand_R.best_score_}\")"
      ],
      "metadata": {
        "id": "FEwYGB-BHc6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating Regression Metrics using GridSearchCV in RandomForestRegressor\n",
        "regression_metrics(y_train,y_train_rand_lgbm_pred,y_test,y_test_rand_lgbm_pred)"
      ],
      "metadata": {
        "id": "iCOPrq2oHxSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "wUZsBtyNH75l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RandomizedSearchCV was still the better option since it is taking very less processing time without compromising the accuracy. So we have mutually decided to use that hyperparameter optimization technique."
      ],
      "metadata": {
        "id": "tT0IQ3WtIFQl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "mq4DnBQfINAG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have tried different parameters for tuning of our LightGBM model and achieved 0.96 r2 score on training dataset, 0.91 on testing set as well that means our model is optimized and not falling under the underfitting or overfitting side. The best parameters obtained by the optimatization is {'n_estimators': [500, 600], 'max_depth': [3,4,5], 'learning_rate':[ 0.01,0.1]}."
      ],
      "metadata": {
        "id": "AhuLrVBDIaly"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ML Model - 5 - XGBoost Regression"
      ],
      "metadata": {
        "id": "GtxFI6U-Il92"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost Regression"
      ],
      "metadata": {
        "id": "EMMjfZNbIxUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the regressor\n",
        "from xgboost import XGBRegressor\n",
        "  \n",
        "# create a regressor object\n",
        "xgbR = XGBRegressor(learning_rate=0.1, max_depth=5) \n",
        "  \n",
        "# fit the regressor with X and Y data\n",
        "xgbR.fit(X_train, y_train)\n",
        "\n",
        "# predict the model\n",
        "y_train_xgbR_pred= xgbR.predict(X_train)\n",
        "y_test_xgbR_pred= xgbR.predict(X_test)"
      ],
      "metadata": {
        "id": "SPF-wOZnI7Ni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating Regression Metrics using RandomForestRegressor\n",
        "regression_metrics(y_train,y_train_xgbR_pred,y_test,y_test_xgbR_pred)"
      ],
      "metadata": {
        "id": "J6Fc20VnJFKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "9EUTEyDhJOFh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer : XGBoost (eXtreme Gradient Boosting) is a Gradiant boosting algorithm and very popular for achieving good accuracies. We have used XGBoost.\n",
        "\n",
        "We got r2 score of 0.94 for testing dataset which is which is excelent and improved as well. At this point of time slightly improvement in MAPE can lead to huge profit to stakeholders and we were very curious and excited at his point of time to further improve the efficiency of our model and for this we have again decided to tune the various hyperparameters of xgboost."
      ],
      "metadata": {
        "id": "yBdcRW9OJdcw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "5EVUOr5NJlmo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost with RandomizedSearchCV"
      ],
      "metadata": {
        "id": "o-zwhJsAJy_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from xgboost import XGBRegressor"
      ],
      "metadata": {
        "id": "SMgQwSadb9jK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n"
      ],
      "metadata": {
        "id": "LhyQb4_ycGdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import ridge regression from sklearn library and RandomizedSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Creating XGBoost instance\n",
        "xgb= XGBRegressor()\n",
        "\n",
        "# Defining parameters\n",
        "parameters={\"learning_rate\":[0.01, 0.1],\"max_depth\":[4,5]}\n",
        "\n",
        "# Train the model\n",
        "xgb_Rand_R= GridSearchCV(xgb,parameters,scoring='neg_mean_squared_error',n_jobs=-1,cv=3,verbose=3)\n",
        "xgb_Rand_R.fit(X_train,y_train)\n",
        "\n",
        "# Predict the output\n",
        "y_train_rand_xgbR_pred = xgb_Rand_R.predict(X_train)\n",
        "y_test_rand_xgbR_pred = xgb_Rand_R.predict(X_test)\n",
        "\n",
        "# Printing the best parameters obtained by GridSearchCV\n",
        "print(f\"The best alpha value found out to be: {xgb_Rand_R.best_params_}\")\n",
        "print(f\"Negative mean square error is: {xgb_Rand_R.best_score_}\")"
      ],
      "metadata": {
        "id": "yc62llHRcakH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating Regression Metrics using GridSearchCV in RandomForestRegressor\n",
        "regression_metrics(y_train,y_train_rand_xgbR_pred,y_test,y_test_rand_xgbR_pred)"
      ],
      "metadata": {
        "id": "H5w0HEyOcnNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting the figure\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.plot(y_test_rand_xgbR_pred, color='Blue')\n",
        "plt.plot(np.array(y_test), color='Red')\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tHvbGrKbcvZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "h2Bq8Zmjc4_E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer : XGboost is a heavy algorithm and takes much processing time with GridSearchCV. So, tuning of hyperparameter with GridSearchCV was a bit complicated task for us. RandomizedSearchCV is excellent hyperparameter optimization technique for this senario. It can take variety of parameters and take the random possible combinations of hyperparameters. So we have used RandomizedSearchCV for hyperparameter tuning."
      ],
      "metadata": {
        "id": "sOmxZt74dBuA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "mcu2sLeodF4z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We have tried different parameters for tuning of our XG Boost model and achieved 0.93 r2 score on training dataset, 0.90 on testing set as well that means our model is optimized and not falling under the underfitting or overfitting side. The best parameters obtained by the optimatization is {'n_estimators': [500, 600], 'max_depth': [3,4,5], 'learning_rate':[ 0.01,0.1]}.\n",
        "\n",
        "Minor improvement in regresson metrics are also significant now as we are moving towards model perfection. With the help of RandomizedSearchCV we got the r2 score of 0.94 (Now 94% of the variance of test set our model is capturing) for test dataset which is 1% higher than without RandomizedSearchCV and the best parameters found out to be{'learning_rate': 0.1, 'max_depth': 13}. Also we have noticed that our MAPE is further reduced and falling under 3% (Minimum error among all models) and on the other hand MSE is also reduced to 12%. We have also seen that on further increasing the max_depth of tree our model is overfitting so above values of parameters are the best combinations."
      ],
      "metadata": {
        "id": "pQ3olb7mdNgf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "yFjtmfE7dgga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since predicting sales over a period of time falls under the category of \"Time series data\" and there are following regression metrics that are required as per our goal of analysis (Predicting future Sales):\n",
        "\n",
        "1. MAE(Mean Absolute Error): This metric calculates the average magnitude of the errors in the predictions, without considering their direction. It has the inverse relation with the accuracy of the model. In regression analysis our aim is to minimise the MAE and ultimately this will create positive business impact.\n",
        "2. RMSE(Root Mean Squared Error): It is the square root of MSE and this is the most widely use regression metric since it has the same units as the original data so it is easy to interpret the magnitude of error.\n",
        "\n",
        "3. MAPE(Mean Absolute Percentage Error): It is calculated by taking the average of the absolute percentage differences between the predicted values and the actual values. This metric is particularly useful when working with time series data(as in our case), as it allows for easy comparison of forecast accuracy across different scales. With the help of MAPE an analyst can easily explain the percentage error to the stakeholders. This metric is considered as one of the most important regression metric in time series data for a positive business impact.\n",
        "4. R2_Score: R2 score(coefficient of determination) is a metric that is widely used in regression analysis because it measures the proportion of the variance in the dependent variable that is explained by the independent variables. R2 score allows analysts to quickly and easily evaluate the goodness of fit of a model and compare different models. It also provides a clear measure of how well the model is explaining the variance in the dependent variable, which can aid in making decisions about model selection and further analysis.\n",
        "\n",
        "5. Adjudusted R2_Score: R2 score(coefficient of determination) is a metric that is widely used in regression analysis because it measures the proportion of the variance in the dependent variable that is explained by the independent variables. Adjusted R2 score allows analysts to quickly and easily evaluate the goodness of fit of a model and compare different models. It also provides a clear measure of how well the model is explaining the variance in the dependent variable, which can aid in making decisions about model selection and further analysis.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IGwEN6d-drRA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "P880iavQerIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing different regression metrics in order to make dataframe and compare them\n",
        "models = [\"Linear_regression\",\"Decision_tree\",\"Random_forest\",\"LightGBM\",\"XGboost\"]\n",
        "\n",
        "r2_r = [.83,0.86,0.88,0.91,0.90]\n",
        "adjusted_r2 = [.82,.86,.88,.90,.90]\n",
        "\n",
        "# Create dataframe from the lists\n",
        "data = {'Models': models, \n",
        "        'R2': r2_r,\n",
        "        'Adjusted R2': adjusted_r2\n",
        "       }\n",
        "metric_df = pd.DataFrame(data)\n",
        "\n",
        "# Printing dataframe\n",
        "metric_df"
      ],
      "metadata": {
        "id": "B2GSqnSDe2x6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have chosen XGboost as our final prediction model with hyperparameters {'learning_rate': 0.1, 'max_depth': 13} as it is very clear from above dataframe that it has given the highest accuracy (97%), least MAPE (3%) and maximum r2 score(0.94) on the testing dataset among all other models."
      ],
      "metadata": {
        "id": "n8QnlkPufCIT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "frs8Ei4TfKLq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost (eXtreme Gradient Boosting) provides an efficient implementation of the gradient boosting framework. It is designed for both linear and tree-based models, and it is useful for large datasets. The basic idea behind XGBoost is to train a sequence of simple models, such as decision trees, and combine their predictions to create a more powerful model. Each tree is trained to correct the errors made by the previous trees in the sequence and this known as boosting.\n",
        "\n",
        "XGBoost uses a technique called gradient boosting to optimize the parameters of the trees. It minimizes the loss function by adjusting the parameters of the trees in a way that reduces the error of the overall model. XGBoost also includes a number of other features, such as regularization, which helps to prevent overfitting, and parallel processing, which allows for faster training times.\n",
        "\n",
        "Although tree based algorithm gives most accurate results but they have less explanability. With the help of some explanabilty tools like LIME and SHAP we can explain our model to the stakeholders."
      ],
      "metadata": {
        "id": "1v41z-ESfXOT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "32Y7fPK4xTmi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project we have dataset that We started with loading the data and then we did Exploratory Data Analysis (EDA), on all the feature of our dataset then analysed our dependent variable ie “Ranted Bike count” and then transform it then null values treatment, feature selection, encoding of categorical columns, and analyse the numeric variable, check the correlation and drop the highly correlated variable and then inhot coding then build the model and extract statistical information that quite useful for the Business prespective\n",
        "\n",
        "Next we have implemented machine learning algorithms like Linear Regression, Lasso, Ridge, Decision Tress, Random Forest, XGBoost . We did some hyperparameter tuning to improve our model performance."
      ],
      "metadata": {
        "id": "27B_UgByfnvO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#observation:\n",
        "\n",
        "* In holiday or non-working days there is demands in rented bikes.\n",
        "* .There is a surge of high demand in the morning 8AM and in evening 6PM as the people might be going to their work at morning 8AM and returing from their work at the evening 6PM.\n",
        "\n",
        "* People prefered more rented bikes in the morning than the evening.\n",
        "* When the rainfall was less, people have booked more bikes except some few cases.\n",
        "\n",
        "* The Temperature, Hour & Humidity are the most important features that positively drive the total rented bikes count.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KM2mXsOgfwVA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eveluation matrics"
      ],
      "metadata": {
        "id": "F8Z9w7uIg6do"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing different regression metrics in order to make dataframe and compare them\n",
        "models = [\"Linear_regression\",\"Decision_tree\",\"Random_forest\",\"LightGBM\",\"XGboost\"]\n",
        "R2_Score_train=[.81,.89,.91,.94,.93]\n",
        "R2_score_test = [.83,0.86,0.88,0.91,0.90]\n",
        "Adjusted_r2_train =[.80, .89,.91,.94,.92 ]\n",
        "Adjusted_r2_test = [.82,.86,.88,.90,.90]\n",
        "\n",
        "# Create dataframe from the lists\n",
        "data = {'Models': models, \n",
        "        'R2 Score Train': R2_Score_train,\n",
        "        'R2 Score Test' : R2_score_test,\n",
        "        'Adjusted R2 Score Train': Adjusted_r2_train,\n",
        "        'Adjusted R2 Score Test' :Adjusted_r2_test\n",
        "       }\n",
        "metric_df = pd.DataFrame(data)\n",
        "\n",
        "# Printing dataframe\n",
        "metric_df"
      ],
      "metadata": {
        "id": "BEpne_54hEZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your EDA Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}